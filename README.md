# Memory Augmented Recurrent Neural Networks For De-novo Drug Generation #
This repository is the code to *Memory Augmented Recurrent Neural Networks For De-novo Drug Generation* by Naveen Suresh, Neelesh CA, Srikumar Subramanian and Dr. Gowri Srinivasa, currently submitted to PLOS One for review.

## Plos One ##
The data is uploaded as a separate zip due to size constraints. Please unzip the file and add it to the root directory.

## Installing requirements ##
Conda is used to install the dependencies. Some of the dependencies like rdkit are harder to install using other means, hence they are not covered in this readme.<br> <br>
### Commands ###
conda create --name environment_name --file requirements.txt <br>
conda activate environment_name

## Folder Structure
* Data -> chembl_22_clean_1576904_sorted_std_final.smi is used by the generator. We have used only half of the available dataset (what is present in full_chembl_22_clean_1576904_sorted_std_final) due to computational constraints. predcitor_data is used by the generator. It is in a zip file due to size restrictions.
* Generators -> contains the source code for the training of generators (stack RNN, NTM, DNC, baseline) and generation of strings.
* Predictor -> contains the source code to train the character level CNN predictor.
* RL -> Code for biasing using Policy Gradient.
* Evaluation -> contains code needed to generate the figures and data(validity, novelty, SA score and lengths) in the paper.
* Output -> contains the strings(more than 1000 for each model) generated by the unbiased generators and generators biased for logP and the number of Benzene Rings.
* 2D Structural Representation -> An application to generate the 2D structural representation of the SMILES strings generated. 
* release -> contains utility scripts used by the generators.

## Running the generator ##

Note: There may be out of memory errors while training the generators. In case it is not possible to increase the amount of VRAM, the batch size would need to be reduced. This can be done by modifying the batch_size variable.<br>
### Generator flags ###
These are present in the run_tasks.py file. <br>
* starting_model_number -> if a model is being loaded, this value can be set to ensure values like generated file number and loss iteration are correct.
* num_train_steps -> The number of iterations for which the model will run
* load -> if an existing model has to be loaded. If 0, it trains from scratch. The path of the model folder can be specified using the PATH_TO_MODEL variable.
* train -> if the model needs to be trained. After training, the model will be saved in a folder specified by the PATH_TO_MODEL variable.
* generate -> if strings need to be generated. Strings will be generated in the folder specified by the "PATH_TO_GENERATED_FILE" variable.
* num_evaluate -> number of iterations after which strings will be generated in the folder specified by the "PATH_TO_GENERATED_FILE" variable. This is useful is comparison of model performance across iterations. e.g. num_train_steps is 45000, and num_evaluate is 5000, the performance across every 5000th iteration can be compared.
* num_strings -> The number of batches of strings generated. The total number of strings in the output file is num_strings * batch_size

For training a model from scratch, set train = 1, and num_train_steps to a large value like 45000. The other variables like num_evaluate and num_strings can be set as required, they are not necessary for training. generate can be = 0 <br>
If a saved model needs to be trained further, set load = 1, and give the appropriate path, along with the flags mentioned above. <br>
To only generate strings from a saved model, set load = 1, give the appropriate path. Set generate = 1. Set num_strings to the value required.<br>

### Code for execution ###
cd Generators/{generator}/src <br>
python run_tasks.py <br>
The loss is written to a "losses.txt" file.

## Running the predictor ##
The predictor reads the Data/predictor_data.csv file. (the zipped file needs to be unzipped) <br>
The CSV file has the canonincal smiles and their corresponding log P and benzene values. <br>
The only variable to be changed is the PROPERTY_COLUMN value. If log P is needed, the value should be ALOGP. If benzene is needed, the value should be Benzene. <br>

### Code for execution ###
cd Predictor/src <br>
python predictor.py <br>

This saves the models in the "model" folder. It saves a model if it's the best at that particular iteration. Hence multiple models are saved, the latest one would have the best validation loss.


## Reinforcement Learning ##

The variables are similar to the generator, hence refer to that generation section.
In addition to that, a predictor is loaded. The PREDICTOR_MODEL_PATH is set for this.

As the properties, number of benzene rings and log P have different magnitudes, the loss coefficients are also different. Early stopping was used to prevent overfitting, hence in addition to the loss coefficient, the value for early stopping also varies.
What was used by us is given below, these can be incremented/decremented as necessary.
For maximization, the loss value is negative, so that a higher value leads to a lower loss. 

Setting the loss in the BuildTModel class <br>
Minimization log p -> self.predicted_loss = 4*tf.math.exp(tf.reduce_sum(predicted)/batch_size)<br>
Maximization log p -> self.predicted_loss = -1*(tf.math.exp((tf.reduce_sum(predicted)/batch_size)/1.5))<br>
Minimization benzene -> self.predicted_loss = 1*(tf.math.exp((tf.reduce_sum(predicted)/batch_size)*2))<br>
Maximization benzene -> self.predicted_loss = -1*(tf.math.exp((tf.reduce_sum(predicted)/batch_size)*1.8))<br>

Setting the value for early stopping in the training loop (e.g. predicted_loss_list[index] > 0.7) <br>
Minimization log p -> > 1.8<br>
Maximization log p -> < 3.8<br>
Minimization benzene -> > 0.7<br>
Maximization benzene -> < 1.65<br>

### Code for execution ###
Store unbiased model files in desired path <br><br>
cd rl/{generator}/src <br>
python run_tasks.py <br>

The saved model can be used to generate strings. (Refer the generation section).

### Executing baseline models ###
Store unbiased model files in desired path <br><br>
cd rl/baseline <br>

Edit the code_runner.sh file to pick the type of baseline models and biasing.<br>
Run the shell script ./code_runner.sh
The models will be saved along with a generated sample of strings used to evaluate the model.

## Evaluation ##
This folder contains the scripts used the generate the graphs/data in the paper/supplementary material. It also contains code for the moses metrics and FCD.
### density_plot ###
This generates the density plots and data seen in the tables for quartiles, average, valid string %, common string %, average length and SAS score. <br>
Set the property option (benzene/logP) <br>
Set the appropriate paths for the files <br>
Select the option, ntm/dnc/stack/unbiased/minimized/maximized <br>

### Code for execution ###
cd Evaluation/density_plot<br>
python src.py<br>

### iteration_interval ###
This generates the valid string %, common string % and jaccard index for the generators at intervals of 5000 iterations <br>

### Code for execution ###
cd Evaluation/iteration_interval<br>
python src.py<br>
This results are written to a file, 'results.csv' <br>

### moses ###
There are some modifications done to the moses code. FCD is computed seperately. Filters are removed as no fitlering was done for our dataset. Scaffold sets are not used since that was not the focus of the project. There was an overflow issue in calculating cos_similarity, that is fixed. There was some data present in the original repository which is removed here to keep it lightweight.<br>
Note - for moses the test set has metrics that can be reused across models. This is stored as an npy file in a zip. This needs to be unzipped to be used. If this file is present, computation will be faster. This path can be set using the pathToStats variable.

### FCD ###
The code is present as a jupyter notebook in Evaluation/FCD/FCD.ipnyb

### Code for execution ###
Install moses - 1. cd Evaluation/moses/moses 2.python setup.py install<br>
Run the code to calculate metrics - 1. cd Evaluation/moses 2.python src.py <br>
The above code writes dictionaries to a text file. A utility to_csv.py is presnt if needed to convert to a csv.


## Viewing 2D Representations of SMILES strings ##
This application displays 2D representations of molecules sampled from the SMILES strings generated by each of the generators. Options are provided to select the preferred generator, biasing (Minimization or Maximization) and the property based on which the generation is biased.

### Steps to run application ###
cd "2D Structural Representation"
python server.py
In the webbrowser navigate to localhost:5000

## Citations ##
The chembl_22_clean_1576904_sorted_std_final.smi file and release folder are taken from Mariya Popova, Olexandr Isayev, Alexander Tropsha. Deep Reinforcement Learning for de-novo Drug Design. Science Advances, 2018, Vol. 4, no. 7, eaap7885. DOI: 10.1126/sciadv.aap7885 (https://github.com/isayev/ReLeaSE). 

The moses code is taken from @article{10.3389/fphar.2020.565644,
  title={{M}olecular {S}ets ({MOSES}): {A} {B}enchmarking {P}latform for {M}olecular {G}eneration {M}odels},
  author={Polykovskiy, Daniil and Zhebrak, Alexander and Sanchez-Lengeling, Benjamin and Golovanov, Sergey and Tatanov, Oktai and Belyaev, Stanislav and Kurbanov, Rauf and Artamonov, Aleksey and Aladinskiy, Vladimir and Veselov, Mark and Kadurin, Artur and Johansson, Simon and  Chen, Hongming and Nikolenko, Sergey and Aspuru-Guzik, Alan and Zhavoronkov, Alex},
  journal={Frontiers in Pharmacology},
  year={2020}
}
(https://github.com/molecularsets/moses)

The FCD code is taken from @misc{preuer2018frechet,
      title={Fr\'echet ChemNet Distance: A metric for generative models for molecules in drug discovery}, 
      author={Kristina Preuer and Philipp Renz and Thomas Unterthiner and Sepp Hochreiter and Günter Klambauer},
      year={2018},
      eprint={1803.09518},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
(https://github.com/bioinf-jku/FCD)
